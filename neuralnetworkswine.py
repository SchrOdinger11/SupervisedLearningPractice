# -*- coding: utf-8 -*-
"""NeuralNetworksWine.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nQmJ9q3fAQzty6zNUAxYa8LtlfTi296y
"""

import numpy as np
from sklearn.model_selection import train_test_split

import pandas as pd

import matplotlib.pyplot  as plt

"""Sigmoid and its Derivatives"""

def f(x):

    return 1 / (1 + np.exp(-x))
def fd(x):
    return x * (1 - x)
def softmax(z):
    e_z = np.exp(z - np.max(z, axis=1, keepdims=True))
    return e_z / np.sum(e_z, axis=1, keepdims=True)
def OneHotEncoded(y):

    classes = np.unique(y)
    totClasses = len(classes)

    oneHotVector = np.zeros((y.shape[0], totClasses))

    labelIndexDict = {}
    for idx, label in enumerate(classes):
        labelIndexDict[label] = idx


    for idx, label in enumerate(y):
        oneHotVector[idx, labelIndexDict[label]] = 1

    return oneHotVector
def featureNormalize(x):

    return (x - np.mean(x,axis =0))/ np.std(x, axis =0)

"""Initialize Weights and cost function"""

#Xavier initialization
def initializeWeights(layers):
    weights = []
    for i in range(len(layers) - 1):
        inputDimension = layers[i] + 1
        outputDimension = layers[i + 1]
        std = np.sqrt(2 / (inputDimension + outputDimension - 1))
        W = np.random.randn(outputDimension, inputDimension) * std
        weights.append(W)
    return weights
def J(a, y, weights, lambdA):
    m = y.shape[0]
    op = a[-1]
    m = y.shape[0]
    if len(y.shape) == 1 or y.shape[1] == 1:

        y = OneHotEncoded(y)
    log_likelihoods = -np.sum(y * np.log(op + 1e-8))
    cost = log_likelihoods / m
    regTerm = lambdA / (2 * m) * sum(np.sum(weight[:, 1:] ** 2) for weight in weights)
    return cost + regTerm

"""Forward Pass Backward Pass and Gradient compute and Update weights"""

def forward(x, weights):
    activations = []
    a = np.concatenate([np.ones((x.shape[0], 1)), x], axis=1)
    activations.append(a)

    for i, W in enumerate(weights):
        z = np.dot(a, W.T)
        if i < len(weights) - 1:
          a = f(z)
        else:
          a = softmax(z)

        if i < len(weights) - 1:
            a = np.concatenate([np.ones((a.shape[0], 1)), a], axis=1)

        activations.append(a)
    return activations

def computeGradients(activations, deltas,lambdA):
    gradients = []
    averageGradients = []

    for i in range(len(deltas)):
        layer_gradients = []

        for j in range(activations[i].shape[0]):
            grad = np.outer(deltas[i][j], activations[i][j])
            layer_gradients.append(grad)

        gradients.append(layer_gradients)

        average_grad = np.mean(layer_gradients, axis=0)


        averageGradients.append(average_grad)


    return gradients,averageGradients

def backward(weights, activations, y,lambdA):
    deltas = []
    gradients = []
    m = y.shape[0]
    if len(y.shape) == 1 or y.shape[1] == 1:

        y = OneHotEncoded(y)

    delta_output = activations[-1] - y
    deltas.append(delta_output)



    for i in reversed(range(len(weights))):
        a = activations[i][:, 1:]

        if i > 0:
            w_next = weights[i]
            delta_next = deltas[0]
            delta_hidden = np.dot(delta_next, w_next[:, 1:]) * fd(a)
            deltas.insert(0, delta_hidden)



    gradients,avgGradients = computeGradients( activations,deltas,lambdA )
    return deltas, avgGradients

def updateWeights(weights, gradients, alpha, lambdaA):

    newWeights = []


    for W, grad in zip(weights, gradients):

        grad=np.array(grad)


        #print("Shape of regularization:", regularization.shape)
        regularization = lambdA * W


        W_new = W - alpha * (grad + regularization)
        newWeights.append(W_new)

    return newWeights

"""Neural Network"""

def trainNetwork(X_train, y_train, layers, lambdA, alpha, epsilon, max_epochs):
    weights = initializeWeights(layers)

    prev_cost = float('inf')

    for epoch in range(max_epochs):
        activations = forward( X_train,weights)

        costFunction = J(activations, y_train, weights, lambdA)

        # if prev_cost - costFunction < epsilon:
        #     print(f'Stopping early at epoch {epoch} due to small change in cost: {prev_cost - costFunction}')
        #     break
        prev_cost = costFunction

        deltas,gradients = backward(weights, activations, y_train,lambdA)
        #print('Running backpropagation')
        # For each layer, print deltas and gradients
        #return weights,gradients,alpha,lambdA

        weights = updateWeights(weights,gradients,alpha,lambdA)


        #print(f'Epoch {epoch}, Cost: {costFunction}')

    return weights

"""Metric Calculations"""

def calculateAccuracy(actualLabels,predictedLabels):
    count=0
    for actual, prediction in zip(actualLabels,predictedLabels):
        if actual == prediction:
            count=count+1
    return count/len(predictedLabels)
def calculatePrecision(actualLabels, predictedLabels):
    labels = set(actualLabels)
    precisionSum = 0

    for c in labels:
        truePositives = 0
        totalPositives = 0
        for actual, prediction in zip(actualLabels, predictedLabels):
            if prediction == c:
                totalPositives += 1
                if actual == prediction:
                    truePositives += 1


        if totalPositives > 0:
            classPrecision = truePositives / totalPositives
            precisionSum += classPrecision



    return precisionSum / len(labels)


def calculateRecall(actualLabels, predictedLabels):
    labels = set(actualLabels)
    recallSum = 0

    for c in labels:
        truePositives = 0
        actualPositives = 0
        for actual, prediction in zip(actualLabels, predictedLabels):
            if actual == c:
                actualPositives += 1
                if prediction == c:
                    truePositives += 1


        if actualPositives > 0:
            classRecall = truePositives / actualPositives
            recallSum += classRecall



    return recallSum / len(labels)


def plotGraph(X_train, Y_train, X_test, y_test):
  increments = [5, 50,100,200,1000, 2000, 3000, 4000, 5000, 6000,7000, 8000, 9000, 10000]
  test_scores = []
  for increment in increments:

      if increment <= len(X_train):
          extended_X_train = X_train[:increment]
          extended_y_train = y_train[:increment]
      else:

          full_copies = increment // len(X_train)
          remainder = increment % len(X_train)
          extended_X_train = np.vstack([X_train]*full_copies + [X_train[:remainder]])
          extended_y_train = np.concatenate([y_train]*full_copies + [y_train[:remainder]])



      trained_weights = trainNetwork(extended_X_train, extended_y_train, layers, lambdA, alpha, epsilon, max_epochs)



      predictions = forward(X_test, trained_weights)
      costFunction = J(predictions, y_test, trained_weights, lambdA)


      test_scores.append(costFunction)
  plt.plot(increments, test_scores)#, marker='o')
  plt.xlabel('Training samples processed')
  plt.ylabel('Loss on Test Set')
  plt.title('Learning Curve')
  plt.grid(True)
  plt.show()

"""Stratified Cross Validation"""

def stratifiedCrossValidation(X, y,  k=10):

    accuracies=[]
    precisions=[]
    recalls=[]
    f1s = []


    labels = np.unique(y)

    classIndex = {}


    for c in labels:
        indices = np.where(y == c)[0]
        classIndex[c] = indices



    foldsIneachClass = {}

    for label, index in classIndex.items():

        shuffle = np.random.permutation(index)

        folds = np.array_split(shuffle, k)

        foldsIneachClass[label] = folds




    for foldNumber in range(k):

        testIndices = []
        trainIndices = []


        for folds in foldsIneachClass.values():
            for index in folds[foldNumber]:
                testIndices.append(index)

        for folds in foldsIneachClass.values():
            for i in range(len(folds)):
                if i != foldNumber:
                    for index in folds[i]:
                        trainIndices.append(index)
        X_train, y_train = X.iloc[trainIndices], y.iloc[trainIndices]
        X_test, y_test = X.iloc[testIndices], y.iloc[testIndices]
        X_train=X_train.to_numpy()
        y_train =y_train.to_numpy()
        X_test= X_test.to_numpy()
        y_test= y_test.to_numpy()
        #1.Test Accuracy: 0.6345158238734091 layers = [X_train.shape[1],1, 3]. #lambdA = 0.1  #alpha = 0.3 #F1 score: 0.512829606690403
        #2 .Test Accuracy: 0.9830065359477125 F1 score: 0.9853157999601514  layers = [X_train.shape[1],32, 3]   lambdA = 0.1  #0.01 alpha = 0.3 #0.07




        layers = [X_train.shape[1],8,8, 3]
        lambdA = 0.1 #0.01
        alpha = 0.95 #0.07
        epsilon = 1e-7
        max_epochs = 100

        network = trainNetwork(X_train, y_train, layers, lambdA, alpha, epsilon, max_epochs)
        predictions  = forward(X_test, network)
        predictions=predictions[-1]

        predicted_labels = np.argmax(predictions, axis=1)+1

        # testAccuracy = accuracy_score(y_test,predicted_labels)#calculateAccuracy(y_test,predicted_labels)
        # testPrecision=precision_score(y_test,predicted_labels, average='weighted')#calculatePrecision(y_test,predicted_labels)
        # testRecall=recall_score(y_test,predicted_labels, average='weighted')#calculateRecall(y_test,predicted_labels)
        # testF1=f1_score(y_test,predicted_labels, average='weighted')#2*testPrecision*testRecall/(testPrecision+testRecall)
        testAccuracy = calculateAccuracy(y_test,predicted_labels)
        testPrecision=calculatePrecision(y_test,predicted_labels)
        testRecall=calculateRecall(y_test,predicted_labels)
        testF1=2*testPrecision*testRecall/(testPrecision+testRecall)
        accuracies.append(testAccuracy)
        f1s.append(testF1)

    return np.mean(accuracies),np.mean(f1s)


wines=pd.read_csv('/content/hw3_wine.csv',sep='\t',header=0)
X = wines.iloc[:, 1:]
y = wines.iloc[:, 0]
X= featureNormalize(X)
accuracy, F1 =stratifiedCrossValidation(X,y)


print(f'Test Accuracy: {accuracy}')
print(f'F1 score: {F1}')

"""Driver Code"""

wines=pd.read_csv('/content/hw3_wine.csv',sep='\t',header=0)
X = wines.iloc[:, 1:]
y = wines.iloc[:, 0]

X= featureNormalize(X)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)




layers = [X_train.shape[1],32,32, 3]
lambdA = 0 #0.01
alpha = 0.3 #0.07
epsilon = 1e-7
max_epochs = 100
X_train=X_train.to_numpy()
y_train =y_train.to_numpy()
X_test= X_test.to_numpy()
y_test= y_test.to_numpy()
# Train the neural network

plotGraph(X_train,y_train, X_test, y_test)

